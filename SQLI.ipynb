{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    global sqlidf\n",
    "    contents=[]\n",
    "    with open(\"sqli.csv\",'r',encoding = 'utf-16') as f:\n",
    "        for line in f:\n",
    "            word=line.split('\\n')\n",
    "            list2 = [x for x in word if x]\n",
    "            list1 = list2[0].rsplit(',',maxsplit=1)\n",
    "            sentence=list1[0][1:]\n",
    "            label=list1[1][:-1]\n",
    "            listx=[sentence,label]\n",
    "            contents += [listx]\n",
    "\n",
    "    contents=contents[1:]\n",
    "    sqlidf = pd.DataFrame(contents,columns=['Sentence','Label'])\n",
    "    \n",
    "load_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlidf['Sentence'] = sqlidf['Sentence'].astype(str)\n",
    "sqlidf['Label']=sqlidf['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sqlidf['Sentence']\n",
    "y=sqlidf['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.to_csv(\"xtest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "posts = vectorizer.fit_transform(X_train).toarray()\n",
    "test_posts = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "(3357, 8820)\n"
     ]
    }
   ],
   "source": [
    "print(posts[5])\n",
    "print(posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vectorizer.vocabulary_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(20, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(10,  activation='tanh'))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8820\n"
     ]
    }
   ],
   "source": [
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                178440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              11264     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 195,035\n",
      "Trainable params: 192,987\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 4s 24ms/step - loss: 0.3035 - accuracy: 0.8545 - val_loss: 0.4530 - val_accuracy: 0.7393\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.0860 - accuracy: 0.9654 - val_loss: 0.4533 - val_accuracy: 0.7393\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.0513 - accuracy: 0.9765 - val_loss: 0.3766 - val_accuracy: 0.7393\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.0479 - accuracy: 0.9790 - val_loss: 0.2505 - val_accuracy: 0.8821\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 8ms/step - loss: 0.0383 - accuracy: 0.9812 - val_loss: 0.1235 - val_accuracy: 0.9655\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 8ms/step - loss: 0.0442 - accuracy: 0.9795 - val_loss: 0.0435 - val_accuracy: 0.9774\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.0432 - accuracy: 0.9796 - val_loss: 0.0467 - val_accuracy: 0.9762\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 14ms/step - loss: 0.0497 - accuracy: 0.9777 - val_loss: 0.0430 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.0526 - accuracy: 0.9773 - val_loss: 0.0478 - val_accuracy: 0.9726\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.0438 - accuracy: 0.9790 - val_loss: 0.0573 - val_accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "classifier_nn = model.fit(posts,y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(test_posts, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer.vocabulary_, open(\"dictionary.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    if pred[i]>0.5:\n",
    "        pred[i]=1\n",
    "    elif pred[i]<=0.5:\n",
    "        pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(tp,tn,fp,fn):\n",
    "    \n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def precision_function(tp,fp):\n",
    "    \n",
    "    precision = tp / (tp+fp)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall_function(tp,fn):\n",
    "    \n",
    "    recall=tp / (tp+fn)\n",
    "    \n",
    "    return recall\n",
    "\n",
    "\n",
    "def confusion_matrix(truth,predicted):\n",
    "    \n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for true,pred in zip(truth,predicted):\n",
    "        if true == 1:\n",
    "            if pred == true:\n",
    "                true_positive += 1\n",
    "            elif pred != true:\n",
    "                false_negative += 1\n",
    "\n",
    "        elif true == 0:\n",
    "            if pred == true:\n",
    "                true_negative += 1\n",
    "            elif pred != true:\n",
    "                false_positive += 1\n",
    "            \n",
    "    accuracy=accuracy_function(true_positive, true_negative, false_positive, false_negative)\n",
    "    precision=precision_function(true_positive, false_positive)\n",
    "    recall=recall_function(true_positive, false_negative)\n",
    "    confusion_matrix_res = [[true_negative, false_negative],[false_positive,true_positive]]\n",
    "    \n",
    "    return (accuracy,\n",
    "            precision,\n",
    "           recall,\n",
    "           confusion_matrix_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy : 0.9726190476190476 \n",
      " Precision : 1.0 \n",
      " Recall : 0.8949771689497716 \n",
      " Confusion matrix: [[621, 23], [0, 196]]\n"
     ]
    }
   ],
   "source": [
    "accuracy,precision,recall, matrix =confusion_matrix(y_test,pred)\n",
    "print(\" Accuracy : {0} \\n Precision : {1} \\n Recall : {2} \\n Confusion matrix: {3}\".format(accuracy, precision, recall, matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# # evaluate loaded model on test data\n",
    "# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# score = loaded_model.evaluate(posts, y_train, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def testing1(querystring):\n",
    "    instance = X_test\n",
    "    instance.iloc[0] = querystring[0]\n",
    "    vocabulary_to_load = pickle.load(open(\"dictionary.pickle\", 'rb'))\n",
    "    loaded_vectorizer = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocabulary_to_load)\n",
    "    loaded_vectorizer._validate_vocabulary()\n",
    "    instance_posts = loaded_vectorizer.transform(instance).toarray()\n",
    "    \n",
    "    pred = loaded_model.predict(instance_posts)\n",
    "    \n",
    "    if pred[0]>0.5:\n",
    "        res=1\n",
    "    else:\n",
    "        res=0\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time =  0.5054571628570557\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#hello world!\n",
    "start = time.time()\n",
    "print(testing1([\"105 OR 1=1\"]))\n",
    "stop = time.time()\n",
    "print(\"time = \",stop-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                177220    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              11264     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 193,815\n",
      "Trainable params: 191,767\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load and evaluate a saved model\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    " \n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# summarize model.\n",
    "model.summary()\n",
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
